{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Search Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a simple example of how to search something on reddit and get some posts and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "RESULT_LIMIT = 10\n",
    "\n",
    "r = praw.Reddit(user_agent='sample_app')\n",
    "nba_subreddit = r.get_subreddit('nba')\n",
    "\n",
    "stephcurry_search = nba_subreddit.search('Stephen Curry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 :: [Question] What did Stephen Curry do that contributed to him being les...\n",
      "https://www.reddit.com/r/nba/comments/3iyowa/question_what_did_stephen_curry_do_that/\n",
      "256\n",
      "232\n"
     ]
    }
   ],
   "source": [
    "post = stephcurry_search.next()\n",
    "\n",
    "print post\n",
    "print post.url\n",
    "print post.ups\n",
    "print post.num_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of List found\n",
      "comments list is filled\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "\n",
    "for i in post.comments:\n",
    "    try:\n",
    "        comment_str = str(i.body.encode('utf-8'))\n",
    "        comments.append(comment_str)\n",
    "    except AttributeError:\n",
    "        print \"End of List found\"\n",
    "        if len(comments) != 0:\n",
    "            print \"comments list is filled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'.', u'the', u',', u'a', u'and', u'of', u'to', u'is', u'in', u'with', u'it', u'that', u'his', u'for', u'on', u'an', u'who', u'by', u'he', u'her', u'\"', u'from', u'as', u'film', u'movie', u'this', u'their', u'but', u'one', u'at', u'the_NEG', u'about', u'a_NEG', u\"there's\", u'to_NEG', u'story', u'are', u'(', u'when', u'so', u'they', u',_NEG', u'be', u')', u'life', u'not', u'you', u'all', u'what', u'into', u'out', u'have', u'she', u'will', u'like', u'even', u'has', u'can', u'only', u'--', u'more', u'its', u':', u';', u'if', u'where', u'search', u'most', u'him', u'look', u\"it's\", u'home', u'them', u'begins', u'make', u'love', u'but_NEG', u'of_NEG', u'two', u'both', u'some', u'which', u'made']\n",
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# Analyze the comments as positive or negative\n",
    "\n",
    "# build a generic analyzer from an example \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "print unigram_feats\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "#for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    #print('{0}: {1}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj\n",
      "subj\n"
     ]
    }
   ],
   "source": [
    "phrase = [u\"you\", u\"suck\", u\"did\", u\"you\", u\"know\", u\"that\"]\n",
    "print sentim_analyzer.classify((phrase))\n",
    "print(classifier.classify(word_feats(['magnificent'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    "\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curry\n",
      "neg\n",
      "Curry and Harden are terrible, terrible defenders, and I'm pretty sure Curry is the worse of the two. Everyone says Curry is in line to be the best PG in the league but I don't think he'll ever be more than top 3 without any defensive skills. Harden has more potential than curry imo but I have a bad feeling he won't reach it. \n",
      "neg\n",
      "Stephen curry is not at the level of george or harden. \n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "#print(classifier.classify(word_feats(['didnt', 'you', 'really', 'think', 'that', 'was', 'great', '?'])))\n",
    "for i in range(0,len(comments)):    \n",
    "    for comment in comments[i].split('\\n'):\n",
    "        if any(i in str(comment) for i in 'Stephen Curry'.lower().split(' ')):\n",
    "            print comment\n",
    "            print(classifier.classify(word_feats(comment)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__getstate__', '__hash__', '__init__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_api_link', '_comment_sort', '_comments', '_comments_by_id', '_extract_more_comments', '_get_json_dict', '_has_fetched', '_info_url', '_insert_comment', '_methods', '_orphaned', '_params', '_populate', '_post_populate', '_replaced_more', '_underscore_names', '_uniq', '_update_comments', 'add_comment', 'approve', 'approved_by', 'archived', 'author', 'author_flair_css_class', 'author_flair_text', 'banned_by', 'clear_vote', 'clicked', 'comments', 'created', 'created_utc', 'delete', 'distinguish', 'distinguished', 'domain', 'downs', 'downvote', 'edit', 'edited', 'from', 'from_api_response', 'from_id', 'from_json', 'from_kind', 'from_url', 'fullname', 'get_duplicates', 'get_flair_choices', 'gild', 'gilded', 'has_fetched', 'hidden', 'hide', 'hide_score', 'id', 'ignore_reports', 'is_self', 'json_dict', 'likes', 'link_flair_css_class', 'link_flair_text', 'locked', 'mark_as_nsfw', 'media', 'media_embed', 'mod_reports', 'name', 'num_comments', 'num_reports', 'over_18', 'permalink', 'quarantine', 'reddit_session', 'refresh', 'removal_reason', 'remove', 'replace_more_comments', 'report', 'report_reasons', 'save', 'saved', 'score', 'secure_media', 'secure_media_embed', 'select_flair', 'selftext', 'selftext_html', 'set_contest_mode', 'set_flair', 'set_suggested_sort', 'short_link', 'stickied', 'sticky', 'subreddit', 'subreddit_id', 'suggested_sort', 'thumbnail', 'title', 'undistinguish', 'unhide', 'unignore_reports', 'unmark_as_nsfw', 'unsave', 'unset_contest_mode', 'unsticky', 'ups', 'upvote', 'url', 'user_reports', 'visited', 'vote']\n"
     ]
    }
   ],
   "source": [
    "print dir(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
